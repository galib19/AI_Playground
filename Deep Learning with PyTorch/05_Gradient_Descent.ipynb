{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMaYGC5jXKjld+jxUmy3LO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"eMjZ_tjlxWYP","executionInfo":{"status":"ok","timestamp":1711440088311,"user_tz":240,"elapsed":4,"user":{"displayName":"Asadullah Hill Galib","userId":"04918573300927859168"}}},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","source":["## Gradient Descent (All Manually)"],"metadata":{"id":"_YK0NOT8x1jP"}},{"cell_type":"code","source":["# f = w*x\n","# f = 2*x\n","\n","X = np.array([1,2,3,4], dtype= np.float32)\n","Y = np.array([2,4,6,8], dtype= np.float32)\n","\n","w = 0.0\n","\n","#model prediction\n","def forward(x):\n","  return w*x\n","# loss = MSE\n","def loss(y, y_predicted):\n","  return ((y_predicted-y)**2).mean()\n","\n","# gradient\n","# MSE = 1/N * (w*x - y)**2\n","# dJ/dw = 1/N 2x (w*x-y)\n","\n","def gradient(x,y, y_predicted):\n","  return np.dot(2*x, y_predicted-y).mean()\n","\n","print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n","\n","#Training\n","learning_rate = 0.01\n","n_iters =  20\n","\n","for epoch in range(n_iters):\n","  #prediction = forward pass\n","  y_pred = forward(X)\n","\n","  #loss\n","  l = loss(Y, y_pred)\n","\n","  #gradients\n","  dw = gradient(X,Y, y_pred)\n","\n","  #update weights\n","  w -= learning_rate* dw\n","\n","  if epoch % 2 == 0:\n","    print(f\"epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}\")\n","\n","    print(f\"Prediction after training: f(5) = {forward(5):.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"agxWLfOoxhSq","executionInfo":{"status":"ok","timestamp":1711440925919,"user_tz":240,"elapsed":168,"user":{"displayName":"Asadullah Hill Galib","userId":"04918573300927859168"}},"outputId":"332d4a34-0ab8-4a19-852d-d5e934603be4"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction before training: f(5) = 0.000\n","epoch 1: w = 1.200, loss = 30.00000000\n","Prediction after training: f(5) = 6.000\n","epoch 3: w = 1.872, loss = 0.76800019\n","Prediction after training: f(5) = 9.360\n","epoch 5: w = 1.980, loss = 0.01966083\n","Prediction after training: f(5) = 9.898\n","epoch 7: w = 1.997, loss = 0.00050331\n","Prediction after training: f(5) = 9.984\n","epoch 9: w = 1.999, loss = 0.00001288\n","Prediction after training: f(5) = 9.997\n","epoch 11: w = 2.000, loss = 0.00000033\n","Prediction after training: f(5) = 10.000\n","epoch 13: w = 2.000, loss = 0.00000001\n","Prediction after training: f(5) = 10.000\n","epoch 15: w = 2.000, loss = 0.00000000\n","Prediction after training: f(5) = 10.000\n","epoch 17: w = 2.000, loss = 0.00000000\n","Prediction after training: f(5) = 10.000\n","epoch 19: w = 2.000, loss = 0.00000000\n","Prediction after training: f(5) = 10.000\n"]}]},{"cell_type":"markdown","source":["## Gradient Descent (using AutoGrad)"],"metadata":{"id":"QiaKXS52z6PQ"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"G9B1s4I_z_33"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# f = w*x\n","# f = 2*x\n","\n","X = torch.tensor([1,2,3,4], dtype= torch.float32)\n","Y = torch.tensor([2,4,6,8], dtype= torch.float32)\n","\n","w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n","\n","#model prediction\n","def forward(x):\n","  return w*x\n","# loss = MSE\n","def loss(y, y_predicted):\n","  return ((y_predicted-y)**2).mean()\n","\n","print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n","\n","#Training\n","learning_rate = 0.01\n","n_iters =  100\n","\n","for epoch in range(n_iters):\n","  #prediction = forward pass\n","  y_pred = forward(X)\n","\n","  #loss\n","  l = loss(Y, y_pred)\n","\n","  #gradients = backward pass\n","  l.backward() # dl/dw\n","\n","  #update weights\n","  with torch.no_grad():\n","    w -= learning_rate*w.grad\n","\n","   # zero gradients\n","  w.grad.zero_()\n","\n","\n","  if epoch % 10 == 0:\n","    print(f\"epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}\")\n","\n","    print(f\"Prediction after training: f(5) = {forward(5):.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711440936696,"user_tz":240,"elapsed":297,"user":{"displayName":"Asadullah Hill Galib","userId":"04918573300927859168"}},"outputId":"d5e25025-f5c6-4531-c3f9-fe9e674282bc","id":"3GhwUkr8z0tv"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction before training: f(5) = 0.000\n","epoch 1: w = 0.300, loss = 30.00000000\n","Prediction after training: f(5) = 1.500\n","epoch 11: w = 1.665, loss = 1.16278565\n","Prediction after training: f(5) = 8.327\n","epoch 21: w = 1.934, loss = 0.04506890\n","Prediction after training: f(5) = 9.671\n","epoch 31: w = 1.987, loss = 0.00174685\n","Prediction after training: f(5) = 9.935\n","epoch 41: w = 1.997, loss = 0.00006770\n","Prediction after training: f(5) = 9.987\n","epoch 51: w = 1.999, loss = 0.00000262\n","Prediction after training: f(5) = 9.997\n","epoch 61: w = 2.000, loss = 0.00000010\n","Prediction after training: f(5) = 10.000\n","epoch 71: w = 2.000, loss = 0.00000000\n","Prediction after training: f(5) = 10.000\n","epoch 81: w = 2.000, loss = 0.00000000\n","Prediction after training: f(5) = 10.000\n","epoch 91: w = 2.000, loss = 0.00000000\n","Prediction after training: f(5) = 10.000\n"]}]},{"cell_type":"markdown","source":["## Gradient Descent (using AutoGrad and PyTorch & Optimizer)"],"metadata":{"id":"RvASuMNo03QA"}},{"cell_type":"code","source":["# 1) design model (input, output size, forward pass)\n","# 2) construct loss and optimizer\n","# 3) training loop (forward pass, backward pass, update weights)"],"metadata":{"id":"unaCiadg1Vl-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"8zEv6lBd1UBi","executionInfo":{"status":"ok","timestamp":1711441232330,"user_tz":240,"elapsed":139,"user":{"displayName":"Asadullah Hill Galib","userId":"04918573300927859168"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# f = w*x\n","# f = 2*x\n","\n","X = torch.tensor([1,2,3,4], dtype= torch.float32)\n","Y = torch.tensor([2,4,6,8], dtype= torch.float32)\n","\n","w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n","\n","#model prediction\n","def forward(x):\n","  return w*x\n","\n","print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n","\n","#Training\n","learning_rate = 0.01\n","n_iters =  100\n","\n","loss  = nn.MSELoss()\n","optimizer = torch.optim.SGD([w], lr=learning_rate)\n","\n","for epoch in range(n_iters):\n","  #prediction = forward pass\n","  y_pred = forward(X)\n","\n","  #loss\n","  l = loss(Y, y_pred)\n","\n","  #gradients = backward pass\n","  l.backward() # dl/dw\n","\n","  #update weights\n","  optimizer.step()\n","\n","   # zero gradients\n","  optimizer.zero_grad()\n","\n","\n","  if epoch % 10 == 0:\n","    print(f\"epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}\")\n","\n","    print(f\"Prediction after training: f(5) = {forward(5):.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3zRmUk-0uUw","executionInfo":{"status":"ok","timestamp":1711441578817,"user_tz":240,"elapsed":171,"user":{"displayName":"Asadullah Hill Galib","userId":"04918573300927859168"}},"outputId":"38067071-7377-4557-dfa0-95a46d6951bf"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction before training: f(5) = 0.000\n","epoch 1: w = 0.300, loss = 30.00000000\n","Prediction after training: f(5) = 1.500\n","epoch 11: w = 1.665, loss = 1.16278565\n","Prediction after training: f(5) = 8.327\n","epoch 21: w = 1.934, loss = 0.04506890\n","Prediction after training: f(5) = 9.671\n","epoch 31: w = 1.987, loss = 0.00174685\n","Prediction after training: f(5) = 9.935\n","epoch 41: w = 1.997, loss = 0.00006770\n","Prediction after training: f(5) = 9.987\n","epoch 51: w = 1.999, loss = 0.00000262\n","Prediction after training: f(5) = 9.997\n","epoch 61: w = 2.000, loss = 0.00000010\n","Prediction after training: f(5) = 10.000\n","epoch 71: w = 2.000, loss = 0.00000000\n","Prediction after training: f(5) = 10.000\n","epoch 81: w = 2.000, loss = 0.00000000\n","Prediction after training: f(5) = 10.000\n","epoch 91: w = 2.000, loss = 0.00000000\n","Prediction after training: f(5) = 10.000\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"kPZw_Icz3hsX"}},{"cell_type":"code","source":["# f = w*x\n","# f = 2*x\n","\n","X = torch.tensor([[1],[2],[3],[4]], dtype= torch.float32)\n","Y = torch.tensor([[2],[4],[6],[8]], dtype= torch.float32)\n","X_test = torch.tensor([5], dtype = torch.float32)\n","\n","n_samples, n_features = X.shape\n","print(n_samples, n_features)\n","\n","input_size = n_features\n","output_size = n_features\n","\n","#model with torch.nn\n","model = nn.Linear(input_size,output_size)\n","\n","#model with class\n","class LinearRegression(nn.Module):\n","  def __init__(self, input_dim, output_dim):\n","    super(LinearRegression, self).__init__()\n","    #define layers\n","    self.lin = nn.Linear(input_dim, output_dim)\n","  def forward(self,x):\n","    return self.lin(x)\n","\n","model = LinearRegression(input_size,output_size) #same as line 14, with more flexibility\n","\n","print(f\"Prediction before training: f(5) = {model(X_test).item():.3f}\")\n","\n","#Training\n","learning_rate = 0.01\n","n_iters =  100\n","\n","loss  = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","for epoch in range(n_iters):\n","  #prediction = forward pass\n","  y_pred = model(X)\n","\n","  #loss\n","  l = loss(Y, y_pred)\n","\n","  #gradients = backward pass\n","  l.backward() # dl/dw\n","\n","  #update weights\n","  optimizer.step()\n","\n","   # zero gradients\n","  optimizer.zero_grad()\n","\n","\n","  if epoch % 10 == 0:\n","    [w,b] = model.parameters()\n","    print(f\"epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}\")\n","\n","    print(f\"Prediction after training: f(5) = {model(X_test).item():.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bPuiykRo2KAV","executionInfo":{"status":"ok","timestamp":1711441899473,"user_tz":240,"elapsed":125,"user":{"displayName":"Asadullah Hill Galib","userId":"04918573300927859168"}},"outputId":"a97a45c3-b755-4b00-fc8e-aa60e3603ca7"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["4 1\n","Prediction before training: f(5) = 4.701\n","epoch 1: w = 1.129, loss = 8.71924210\n","Prediction after training: f(5) = 5.564\n","epoch 11: w = 1.807, loss = 0.23126762\n","Prediction after training: f(5) = 9.177\n","epoch 21: w = 1.918, loss = 0.01133283\n","Prediction after training: f(5) = 9.762\n","epoch 31: w = 1.937, loss = 0.00533120\n","Prediction after training: f(5) = 9.859\n","epoch 41: w = 1.942, loss = 0.00488270\n","Prediction after training: f(5) = 9.878\n","epoch 51: w = 1.944, loss = 0.00459493\n","Prediction after training: f(5) = 9.884\n","epoch 61: w = 1.945, loss = 0.00432739\n","Prediction after training: f(5) = 9.887\n","epoch 71: w = 1.947, loss = 0.00407551\n","Prediction after training: f(5) = 9.891\n","epoch 81: w = 1.949, loss = 0.00383830\n","Prediction after training: f(5) = 9.894\n","epoch 91: w = 1.950, loss = 0.00361489\n","Prediction after training: f(5) = 9.897\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ra2ukMFe33f1"},"execution_count":null,"outputs":[]}]}